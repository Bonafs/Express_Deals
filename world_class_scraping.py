#!/usr/bin/env python3
"""
World-Class Web Scraping Implementation
Implementing the 10 Best Web Scraping Strategies for Express Deals
"""

import os
import django
import requests
import time
import random
import json
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import logging
from datetime import datetime

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'express_deals.settings')
django.setup()

from scraping.models import ScrapeTarget, ScrapeJob
from products.models import Product, Category

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WorldClassScraper:
    """
    Implements all 10 best practices for web scraping
    """
    
    def __init__(self):
        # Strategy 1: Thorough Planning
        self.objectives = {
            'target_products_per_retailer': 5,
            'max_pages_per_search': 3,
            'data_format': 'django_models',
            'success_threshold': 0.7
        }
        
        # Strategy 4: Proxy Rotation
        self.proxy_pool = [
            {'http': 'http://proxy1.example.com:8080'},
            {'http': 'http://proxy2.example.com:8080'},
            # Add more proxies as needed
        ]
        self.current_proxy_index = 0
        
        # Strategy 5: User-Agent Rotation
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # Strategy 9: URL Queue and Checkpointing
        self.url_queue = []
        self.processed_urls = set()
        self.checkpoint_file = 'scraping_checkpoint.json'
        
        # Strategy 10: Data Validation
        self.validation_rules = {
            'min_price': 0.01,
            'max_price': 50000.0,
            'min_name_length': 3,
            'max_name_length': 200
        }
        
        self.session = requests.Session()
        
    def strategy_1_plan_project(self, target):
        """Strategy 1: Plan scraping project thoroughly"""
        logger.info(f"üìã Planning scraping for {target.name}")
        
        plan = {
            'target_site': target.name,
            'base_url': target.base_url,
            'search_queries': ['laptop', 'phone', 'headphones', 'book', 'shirt'],
            'expected_selectors': {
                'product': target.product_selector,
                'title': target.title_selector,
                'price': target.price_selector,
                'image': target.image_selector
            },
            'data_format': 'Product model instances',
            'success_metrics': {
                'min_products': 3,
                'max_errors': 2
            }
        }
        
        logger.info(f"‚úÖ Project planned: {plan['success_metrics']}")
        return plan
    
    def strategy_2_check_robots_txt(self, base_url):
        """Strategy 2: Respect robots.txt and terms of service"""
        try:
            robots_url = urljoin(base_url, '/robots.txt')
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            
            # Check if our user agent can fetch the search pages
            user_agent = random.choice(self.user_agents)
            can_fetch = rp.can_fetch(user_agent, '/search')
            
            logger.info(f"ü§ñ Robots.txt check for {base_url}: {'‚úÖ Allowed' if can_fetch else '‚ùå Blocked'}")
            return can_fetch
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not check robots.txt for {base_url}: {e}")
            return True  # Assume allowed if can't check
    
    def strategy_3_check_api_availability(self, target):
        """Strategy 3: Prefer APIs when available"""
        # Common API endpoints to check
        api_endpoints = [
            '/api/v1/products',
            '/api/products',
            '/api/search',
            '/api/v2/search',
            '/rest/products',
            '/graphql'
        ]
        
        for endpoint in api_endpoints:
            try:
                api_url = urljoin(target.base_url, endpoint)
                response = self.session.get(api_url, timeout=5)
                
                if response.status_code == 200 and 'json' in response.headers.get('content-type', ''):
                    logger.info(f"üéØ API discovered at {api_url}")
                    return api_url
                    
            except Exception:
                continue
        
        logger.info(f"üìÑ No API found for {target.name}, using HTML scraping")
        return None
    
    def strategy_4_rotate_proxy(self):
        """Strategy 4: Rotate proxies"""
        if not self.proxy_pool:
            return None
            
        proxy = self.proxy_pool[self.current_proxy_index % len(self.proxy_pool)]
        self.current_proxy_index += 1
        logger.info(f"üîÑ Using proxy: {proxy}")
        return proxy
    
    def strategy_5_rotate_user_agent(self):
        """Strategy 5: Cycle through User-Agent headers"""
        user_agent = random.choice(self.user_agents)
        self.session.headers.update({'User-Agent': user_agent})
        logger.info(f"üë§ User-Agent: {user_agent[:50]}...")
        return user_agent
    
    def strategy_6_simulate_human_behavior(self):
        """Strategy 6: Simulate human browsing behavior"""
        # Random delay between 2-10 seconds
        delay = random.uniform(2, 10)
        logger.info(f"‚è±Ô∏è Human-like delay: {delay:.1f}s")
        time.sleep(delay)
        
        # Simulate additional headers
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-GB,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def strategy_7_use_headless_browser_if_needed(self, url):
        """Strategy 7: Use headless browsers for JS-heavy sites"""
        # Check if page needs JavaScript rendering
        try:
            response = self.session.get(url, timeout=10)
            if 'javascript' in response.text.lower() and len(response.text) < 1000:
                logger.info(f"üé≠ Page may need JavaScript rendering: {url}")
                # In production, would use Selenium/Playwright here
                return True
        except Exception:
            pass
        return False
    
    def strategy_8_handle_captcha(self, response):
        """Strategy 8: CAPTCHA detection and handling"""
        captcha_indicators = ['captcha', 'recaptcha', 'hcaptcha', 'challenge']
        
        for indicator in captcha_indicators:
            if indicator in response.text.lower():
                logger.warning(f"üõ°Ô∏è CAPTCHA detected on page")
                # In production, would integrate with 2captcha or similar service
                return True
        
        return False
    
    def strategy_9_manage_url_queue(self, urls_to_add=None, checkpoint=False):
        """Strategy 9: Maintain persistent URL queue and checkpointing"""
        if urls_to_add:
            for url in urls_to_add:
                if url not in self.processed_urls:
                    self.url_queue.append(url)
        
        if checkpoint:
            checkpoint_data = {
                'timestamp': datetime.now().isoformat(),
                'url_queue': self.url_queue,
                'processed_urls': list(self.processed_urls),
                'current_proxy_index': self.current_proxy_index
            }
            
            with open(self.checkpoint_file, 'w') as f:
                json.dump(checkpoint_data, f)
            
            logger.info(f"üíæ Checkpoint saved: {len(self.url_queue)} queued, {len(self.processed_urls)} processed")
    
    def strategy_10_validate_data(self, product_data):
        """Strategy 10: Validate and clean parsed data"""
        errors = []
        
        # Validate price
        try:
            price = float(product_data.get('price', 0))
            if not (self.validation_rules['min_price'] <= price <= self.validation_rules['max_price']):
                errors.append(f"Price {price} out of range")
        except (ValueError, TypeError):
            errors.append("Invalid price format")
        
        # Validate name
        name = product_data.get('name', '')
        if not (self.validation_rules['min_name_length'] <= len(name) <= self.validation_rules['max_name_length']):
            errors.append(f"Name length {len(name)} invalid")
        
        # Check for required fields
        required_fields = ['name', 'price']
        for field in required_fields:
            if not product_data.get(field):
                errors.append(f"Missing required field: {field}")
        
        if errors:
            logger.warning(f"‚ö†Ô∏è Data validation errors: {errors}")
            return False, errors
        
        logger.info(f"‚úÖ Data validation passed for: {name}")
        return True, []
    
    def scrape_with_all_strategies(self, target, search_query):
        """Main scraping method implementing all 10 strategies"""
        logger.info(f"üöÄ Starting world-class scraping for {target.name}")
        
        # Strategy 1: Plan the project
        plan = self.strategy_1_plan_project(target)
        
        # Strategy 2: Check robots.txt
        if not self.strategy_2_check_robots_txt(target.base_url):
            logger.warning(f"‚ùå Robots.txt blocks scraping for {target.name}")
            return []
        
        # Strategy 3: Check for APIs
        api_url = self.strategy_3_check_api_availability(target)
        if api_url:
            # Would implement API scraping here
            pass
        
        # Strategy 4 & 5: Setup session with proxy and user agent
        proxy = self.strategy_4_rotate_proxy()
        self.strategy_5_rotate_user_agent()
        
        # Strategy 6: Human-like behavior
        self.strategy_6_simulate_human_behavior()
        
        search_url = target.search_url_template.replace('{query}', search_query)
        products_found = []
        
        try:
            # Strategy 9: Add to URL queue
            self.strategy_9_manage_url_queue([search_url])
            
            response = self.session.get(search_url, proxies=proxy, timeout=15)
            
            # Strategy 8: Check for CAPTCHA
            if self.strategy_8_handle_captcha(response):
                logger.warning(f"‚ö†Ô∏è CAPTCHA detected, skipping {target.name}")
                return []
            
            # Strategy 7: Check if JS rendering needed
            if self.strategy_7_use_headless_browser_if_needed(search_url):
                logger.info("Would use headless browser in production")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find products using target selectors
                product_elements = soup.select(target.product_selector)
                logger.info(f"üì¶ Found {len(product_elements)} product elements")
                
                for element in product_elements[:plan['success_metrics']['min_products']]:
                    try:
                        # Extract product data
                        name_elem = element.select_one(target.title_selector)
                        price_elem = element.select_one(target.price_selector)
                        
                        if name_elem and price_elem:
                            product_data = {
                                'name': name_elem.get_text(strip=True),
                                'price': self._extract_price(price_elem.get_text(strip=True)),
                                'source': target.name
                            }
                            
                            # Strategy 10: Validate data
                            is_valid, errors = self.strategy_10_validate_data(product_data)
                            
                            if is_valid:
                                products_found.append(product_data)
                                logger.info(f"‚úÖ Valid product: {product_data['name']}")
                            else:
                                logger.warning(f"‚ùå Invalid product data: {errors}")
                    
                    except Exception as e:
                        logger.error(f"Error processing product element: {e}")
                        continue
            
            # Strategy 9: Mark URL as processed and checkpoint
            self.processed_urls.add(search_url)
            self.strategy_9_manage_url_queue(checkpoint=True)
            
        except Exception as e:
            logger.error(f"Error scraping {target.name}: {e}")
        
        logger.info(f"üéØ Completed scraping {target.name}: {len(products_found)} products found")
        return products_found
    
    def _extract_price(self, price_text):
        """Extract numeric price from text"""
        import re
        
        # Remove currency symbols and clean text
        cleaned = re.sub(r'[¬£$‚Ç¨,]', '', price_text)
        
        # Find price pattern
        price_match = re.search(r'(\d+\.?\d*)', cleaned)
        
        if price_match:
            try:
                return float(price_match.group(1))
            except ValueError:
                pass
        
        return 0.0

def run_world_class_scraping():
    """Execute world-class scraping on our targets"""
    
    print("üåü WORLD-CLASS WEB SCRAPING IMPLEMENTATION")
    print("=" * 60)
    print("Implementing all 10 best practices for commercial scraping")
    
    scraper = WorldClassScraper()
    targets = ScrapeTarget.objects.filter(status='active')[:3]  # Limit for demo
    
    all_products = []
    
    for target in targets:
        search_queries = ['laptop', 'book', 'phone']
        
        for query in search_queries[:1]:  # One query per target for demo
            products = scraper.scrape_with_all_strategies(target, query)
            
            # Create Product instances
            for product_data in products:
                try:
                    product = Product.objects.create(
                        name=product_data['name'][:200],
                        description=f"Scraped from {product_data['source']} using world-class methods",
                        price=product_data['price'],
                        category=target.category,
                        stock_status='in_stock',
                        is_active=True,
                        is_featured=True,
                    )
                    
                    all_products.append(product)
                    print(f"‚úÖ Created: {product.name} - ¬£{product.price}")
                    
                except Exception as e:
                    print(f"‚ùå Error creating product: {e}")
            
            # Human-like delay between queries
            time.sleep(random.uniform(3, 8))
    
    print(f"\nüéØ WORLD-CLASS SCRAPING COMPLETE!")
    print(f"   Products created: {len(all_products)}")
    print(f"   Total products in database: {Product.objects.count()}")
    
    return all_products

if __name__ == "__main__":
    run_world_class_scraping()
